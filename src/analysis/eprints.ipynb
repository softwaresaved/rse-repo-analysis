{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/raw/eprints/eprints_repos.txt\", \"r\") as f:\n",
    "    eprints_repos = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/raw/eprints\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging info from all repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for eprints_repo in eprints_repos:\n",
    "    try:\n",
    "        urls = pd.read_csv(os.path.join(data_dir, \"cleaned_repo_urls\", f\"cleaned_urls_{eprints_repo}_2010-_github.com.csv\"))\n",
    "        metadata = pd.read_csv(os.path.join(data_dir, \"publication_urls\", f\"extracted_pdf_urls_{eprints_repo}_2010-.csv\"))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    metadata[\"date\"] = pd.to_datetime(metadata.date, errors=\"coerce\")\n",
    "    metadata[\"year\"] = metadata.date.dt.year\n",
    "    df = pd.merge(urls, metadata, how=\"right\", on=[\"title\", \"author_for_reference\", \"pdf_url\"])\n",
    "    df[\"eprints_repo\"] = eprints_repo\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)  # might contain duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df, remove_duplicate_pdf_mentions, remove_duplicate_pub_mentions):\n",
    "    \"\"\"Perform data cleaning steps.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataset\n",
    "        remove_duplicate_pdf_mentions (bool): if True, drop duplicate links in one PDF\n",
    "        remove_duplicate_pub_mentions (bool): if True, drop duplicate links ocurring on the same page in different PDF links (across repos), and also all duplicates of publications with no links\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: cleaned dataset\n",
    "    \"\"\"\n",
    "    df = df[df.year <= datetime.now().year]\n",
    "    df = df.astype({\"year\": int})\n",
    "    # drop duplicate links in one PDF\n",
    "    if remove_duplicate_pdf_mentions:\n",
    "        df = df.drop_duplicates(subset=[\"title\", \"author_for_reference\", \"pdf_url\", \"github_user_cleaned_url\"])\n",
    "    # drop duplicate links ocurring on the same page in different PDF links (across repos), and also all duplicates of publications with no github links\n",
    "    if remove_duplicate_pub_mentions:\n",
    "        df = df.drop_duplicates(subset=[\"title\", \"author_for_reference\", \"page_no\", \"github_user_cleaned_url\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataset(df, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of GitHub URLs in ePrints per year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "colors = ['tab:blue', 'tab:orange']\n",
    "urls_per_year = df.groupby(\"year\")['github_user_cleaned_url'].count()\n",
    "urls_per_year.plot(\n",
    "    ax=ax1,\n",
    "    xlabel=\"year\",\n",
    "    title=\"Total number of GitHub URLs in ePrints publications\",\n",
    "    marker=\"x\",\n",
    "    color=colors[0],\n",
    "    grid=True\n",
    ")\n",
    "ax1.tick_params(axis='y', labelcolor=colors[0])\n",
    "ax1.set_ylabel(\"URL count\", color=colors[0])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "pubs_per_year = df.groupby(\"year\")['pdf_url'].nunique()\n",
    "pubs_per_year.plot(\n",
    "    ax=ax2,\n",
    "    color=colors[1],\n",
    "    marker=\"x\"\n",
    ")\n",
    "ax2.tick_params(axis='y', labelcolor=colors[1])\n",
    "ax2.set_ylabel(\"publication count\", color=colors[1])\n",
    "fig.savefig(\"../../data/derived/plots/overall/github_in_eprints.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average number of GitHub URLs in an ePrints publication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_per_publication = df.groupby([\"year\", \"title\", \"author_for_reference\", \"pdf_url\"])[\"github_user_cleaned_url\"].count().rename(\"url_count\")\n",
    "urls_per_publication.groupby(\"year\").mean().plot(\n",
    "    xlabel=\"year\",\n",
    "    ylabel=\"mean\",\n",
    "    title=\"Average number of GitHub URLs in an ePrints publication\",\n",
    "    marker=\"x\",\n",
    "    grid=True\n",
    ")\n",
    "plt.savefig(\"../../data/derived/plots/overall/avg_github_in_eprints.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of publications in ePrints that have at least one GitHub URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_one_url = (urls_per_publication > 0).rename(\"min_one_url\").groupby(\"year\").value_counts().rename(\"pub_count\").reset_index()\n",
    "min_one_url = min_one_url.set_index([\"year\", \"min_one_url\"])\n",
    "new_index = pd.MultiIndex.from_product(min_one_url.index.levels)\n",
    "min_one_url = min_one_url.reindex(new_index).fillna(0).astype(int)\n",
    "min_one_url = min_one_url.merge(min_one_url.groupby(\"year\").pub_count.sum().rename(\"total_pubs\"), left_on=\"year\", right_index=True)\n",
    "percentages = min_one_url[\"pub_count\"] / min_one_url[\"total_pubs\"] * 100\n",
    "\n",
    "#plt.figure(figsize=(6, 9))\n",
    "ax = sns.barplot(\n",
    "    data=min_one_url.reset_index(),\n",
    "    x=\"year\",\n",
    "    y=\"pub_count\",\n",
    "    hue=\"min_one_url\"\n",
    ")\n",
    "ax.set(yscale=\"log\", ylabel=\"log of publication count\")\n",
    "ax.set_ylim([0, 10**6])\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "ax.bar_label(ax.containers[0], labels=[f\" {p:.2f}%\" for p in percentages[::2]], rotation=90)\n",
    "ax.bar_label(ax.containers[1], labels=[f\" {p:.2f}%\" for p in percentages[1::2]], rotation=90)\n",
    "\n",
    "leg = ax.get_legend()\n",
    "leg.set_title(\"no. GitHub links in publication\")\n",
    "new_labels = [\"0\", \"> 0\"]\n",
    "for t, l in zip(leg.texts, new_labels):\n",
    "    t.set_text(l)\n",
    "\n",
    "plt.title(\"Publications with at least one GitHub URL\")\n",
    "plt.savefig(\"../../data/derived/plots/overall/min_one_github_in_eprints.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_one_url.reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
